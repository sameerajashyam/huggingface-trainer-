# -*- coding: utf-8 -*-
"""4.ForwardPass+device Handling+Loss Computation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pM5gBn0tMJ0B4zUqWJWsc7cmKe5envPQ
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from torch.nn import functional as F
import torch.nn as nn

"""torch: Core PyTorch library for tensors and GPU support.

AutoTokenizer: Automatically selects the right tokenizer for the model.

AutoModelForSequenceClassification: Loads a BERT-like model ready for classification.

DataLoader, Dataset: PyTorch utilities to load data in batches.

functional and nn: For using built-in loss functions and layers.


"""

# 1. Setup: Tokenizer and Model
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

"""model_name: Specifies the pretrained model to use.

tokenizer: Converts raw text into input IDs + attention masks.

model: Loads a classification head on top of DistilBERT with 2 output labels.


"""

# 2. Device Handling
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""Detects whether GPU (CUDA) is available.

Moves the model to the correct device for training.


"""

# 3. Dummy Dataset
class DummyDataset(Dataset):
    def __init__(self, tokenizer):
        self.samples = ["hello world", "huggingface rocks", "open source is future", "transformers rule"]
        self.labels = [0, 1, 0, 1]
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        encoded = self.tokenizer(self.samples[idx], truncation=True, padding="max_length", max_length=16, return_tensors="pt")
        item = {key: val.squeeze(0) for key, val in encoded.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

"""__init__: Initializes some sample text and binary labels (0 or 1).

__len__: Returns dataset length.

__getitem__: Tokenizes a sentence, adds label, returns a dictionary with:

input_ids, attention_mask, and labels.


"""

dataset = DummyDataset(tokenizer)
dataloader = DataLoader(dataset, batch_size=2)

# 4. Loss Function
loss_fn = nn.CrossEntropyLoss()

"""Wraps the dataset into a DataLoader that gives batches of size 2.


"""

# 5. Forward Pass + Loss Computation Loop
model.train()
for batch in dataloader:
    # Move to device
    batch = {k: v.to(device) for k, v in batch.items()}

    # Forward pass
    outputs = model(input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"])

    # outputs.loss is computed inside HF model, but letâ€™s also do it manually
    logits = outputs.logits
    labels = batch["labels"]
    loss = loss_fn(logits, labels)

    # Print for tracking
    print(f"Logits: {logits}")
    print(f"Labels: {labels}")
    print(f"Loss: {loss.item()}")

    # Backward + Optimizer logic would go here
    # loss.backward()
    # optimizer.step()
    # optimizer.zero_grad()

